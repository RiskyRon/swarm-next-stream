## frontend/app/layout.tsx

```typescript
import type { Metadata } from "next";
import localFont from "next/font/local";
import "./globals.css";

const geistSans = localFont({
  src: "./fonts/GeistVF.woff",
  variable: "--font-geist-sans",
  weight: "100 900",
});
const geistMono = localFont({
  src: "./fonts/GeistMonoVF.woff",
  variable: "--font-geist-mono",
  weight: "100 900",
});

export const metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}
      >
        {children}
      </body>
    </html>
  );
}

```

## frontend/app/page.tsx

```typescript
import ChatBot from '@/components/Chatbot';

export default function Home() {
  return (
    <main className="flex min-h-screen flex-col items-center justify-between p-24">
      <ChatBot />
    </main>
  );
}
```

## frontend/app/api/chat/route.ts

```typescript
import { NextRequest, NextResponse } from 'next/server';

export async function POST(req: NextRequest) {
  const body = await req.json();

  const response = await fetch('http://localhost:8000/chat', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(body),
  });

  const data = await response.json();

  return NextResponse.json(data);
}
```

## frontend/components/CustomMarkdown.tsx

```typescript
import React from 'react';
import ReactMarkdown from 'react-markdown';
import { CustomCodeBlock } from '@/components/CustomCodeBlock';

interface CustomMarkdownProps {
  content: string;
}

export const CustomMarkdown: React.FC<CustomMarkdownProps> = ({ content }) => (
  <ReactMarkdown
    className="prose prose-sm dark:prose-invert max-w-none [&>*:first-child]:mt-0 [&>*:last-child]:mb-0"
    components={{
      code: CustomCodeBlock,
      p: ({ children }) => <p className="mb-4 last:mb-0">{children}</p>,
      ul: ({ children }) => <ul className="list-disc pl-6 mb-4 last:mb-0">{children}</ul>,
      ol: ({ children }) => <ol className="list-decimal pl-6 mb-4 last:mb-0">{children}</ol>,
      li: ({ children }) => <li className="mb-2 last:mb-0">{children}</li>,
      h1: ({ children }) => <h1 className="text-2xl font-bold mb-4 mt-6">{children}</h1>,
      h2: ({ children }) => <h2 className="text-xl font-bold mb-3 mt-5">{children}</h2>,
      h3: ({ children }) => <h3 className="text-lg font-bold mb-2 mt-4">{children}</h3>,
      blockquote: ({ children }) => <blockquote className="border-l-4 border-gray-300 pl-4 italic my-4">{children}</blockquote>,
    }}
  >
    {content}
  </ReactMarkdown>
);

```

## frontend/components/Chatbot.tsx

```typescript
// frontend/components/Chatbot.tsx
'use client'

import React, { useRef, useCallback, useMemo, useState } from 'react';
import { Button } from "@/components/ui/button";
import { Textarea } from "@/components/ui/textarea";
import { Card, CardHeader, CardTitle, CardContent, CardFooter } from "@/components/ui/card";
import { ScrollArea } from "@/components/ui/scroll-area";
import { Loader2, Send, Trash2, Paperclip } from 'lucide-react';
import ChatMessage from '@/components/ChatMessage';
import { useWebSocket } from '@/lib/hooks/useWebSocket';

const ChatBot: React.FC = () => {
  const [input, setInput] = useState('');
  const scrollAreaRef = useRef<HTMLDivElement>(null);
  const messageEndRef = useRef<HTMLDivElement>(null);

  const {
    messages,
    isLoading,
    isConnected,
    currentAgent,
    sendMessage,
    clearMessages
  } = useWebSocket('ws://localhost:8000/ws');

  const scrollToBottom = useCallback(() => {
    if (messageEndRef.current) {
      messageEndRef.current.scrollIntoView({ behavior: 'smooth' });
    }
  }, []);

  React.useEffect(() => {
    scrollToBottom();
  }, [messages, isLoading, scrollToBottom]);

  const handleSubmit = useCallback(() => {
    if (!input.trim()) return;
    sendMessage(input);
    setInput('');
  }, [input, sendMessage]);

  const handleUpload = useCallback(() => {
    console.log("File upload triggered");
    // Implement file upload logic here
  }, []);

  const handleKeyDown = useCallback((e: React.KeyboardEvent<HTMLTextAreaElement>) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSubmit();
    }
  }, [handleSubmit]);

  const messageElements = useMemo(() => (
    messages.map(message => (
      <ChatMessage key={message.id} message={message} />
    ))
  ), [messages]);

  return (
    <div className="flex flex-col h-screen w-4/5 max-w-[55rem] mx-auto max-h-[90vh] rounded-lg overflow-hidden">
      <Card className="flex flex-col h-full">
        <CardHeader className="flex-shrink-0">
          <div className="flex justify-between items-center">
            <CardTitle>AI Chatbot {currentAgent && `(${currentAgent})`}</CardTitle>
            <Button variant="outline" size="icon" onClick={clearMessages} aria-label="Clear chat history">
              <Trash2 className="h-4 w-4" />
            </Button>
          </div>
        </CardHeader>
        <CardContent className="flex-grow overflow-hidden p-0">
          <ScrollArea 
            className="h-full p-4 [&_.scrollbar-thumb]:bg-muted-foreground/50 [&_.scrollbar-thumb]:hover:bg-muted-foreground/80" 
            ref={scrollAreaRef}
          >
            {messageElements}
            {isLoading && (
              <div className="flex justify-start mb-4">
                <div className="bg-secondary text-secondary-foreground rounded-lg p-3 flex items-center">
                  <Loader2 className="h-4 w-4 animate-spin mr-2" />
                </div>
              </div>
            )}
            <div ref={messageEndRef} />
          </ScrollArea>
        </CardContent>
        <CardFooter className="flex-shrink-0 p-2">
          <form onSubmit={(e) => { e.preventDefault(); handleSubmit(); }} className="w-full">
            <div className="relative w-full">
              <Textarea
                value={input}
                onChange={(e) => {
                  setInput(e.target.value);
                  e.target.style.height = 'auto';
                  e.target.style.height = `${e.target.scrollHeight}px`;
                }}
                onKeyDown={handleKeyDown}
                placeholder="Type your message..."
                className="pr-24 border resize-none rounded-md w-full overflow-hidden"
                rows={1}
                aria-label="Chat input"
                style={{ minHeight: '7.5rem', maxHeight: '30rem' }}
              />
              <div className="absolute bottom-2 right-2 flex space-x-2">
                <Button
                  type="button"
                  variant="ghost"
                  size="icon"
                  className="h-8 w-8"
                  onClick={handleUpload}
                  disabled={isLoading || !isConnected}
                >
                  <Paperclip className="h-4 w-4" />
                  <span className="sr-only">Attach file</span>
                </Button>
                <Button 
                  type="submit" 
                  variant="ghost" 
                  size="icon" 
                  className="h-8 w-8"
                  disabled={isLoading || !isConnected}
                >
                  <Send className="h-4 w-4" />
                  <span className="sr-only">Send message</span>
                </Button>
              </div>
            </div>
          </form>
        </CardFooter>
      </Card>
    </div>
  );
};

export default ChatBot;

```

## frontend/components/ChatMessage.tsx

```typescript
import React from 'react';
import type { Message } from '@/lib/hooks/useWebSocket';
import { CustomMarkdown } from '@/components/CustomMarkdown';

interface ChatMessageProps {
  message: Message;
}

export const ChatMessage: React.FC<ChatMessageProps> = React.memo(({ message }) => (
  <div className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'} mb-4`}>
    <div className={`flex items-start pt-5 gap-2 max-w-[100%] ${message.role === 'user' ? 'flex-row-reverse' : 'flex-row'}`}>
      <div className="flex-shrink-0">
        <img
          src={message.role === 'user' ? '/images/user-avatar.png' : '/images/bot-avatar.png'}
          alt={`${message.role} avatar`}
          className="w-10 h-10 rounded-full"
        />
      </div>
      <div className={`rounded-lg p-3 ${
        message.role === 'user' 
          ? 'bg-secondary text-secondary-foreground border' 
          : 'bg-secondary text-secondary-foreground'
      }`}>
        <CustomMarkdown content={message.content} />
        <p className="text-xs mt-2 opacity-50">
          {message.timestamp.toLocaleTimeString()}
        </p>
      </div>
    </div>
  </div>
));

ChatMessage.displayName = 'ChatMessage';

export default ChatMessage;

```

## frontend/components/CustomCodeBlock.tsx

```typescript
import React from 'react';
import { Prism as SyntaxHighlighter } from 'react-syntax-highlighter';
import { vscDarkPlus } from 'react-syntax-highlighter/dist/esm/styles/prism';

const customCodeStyle = {
  ...vscDarkPlus,
};

const customWrapperStyle = {
  'pre[class*="language-"]': {
    ...vscDarkPlus['pre[class*="language-"]'],
    padding: '1rem',
    margin: '0.5rem 0',
    background: 'hsl(var(--secondary))',
    border: '1px solid hsl(var(--border))',
    borderRadius: 'calc(var(--radius) - 2px)',
  },
  ':not(pre) > code[class*="language-"]': {
    ...vscDarkPlus[':not(pre) > code[class*="language-"]'],
    background: 'hsl(var(--secondary))',
    padding: '0.2em 0.4em',
    borderRadius: 'calc(var(--radius) - 4px)',
  }
};

export const CustomCodeBlock = ({ inline, className, children, ...props }: any) => {
  const match = /language-(\w+)/.exec(className || '');
  return !inline && match ? (
    <div className="relative rounded-md overflow-hidden my-4">
      <SyntaxHighlighter
        style={customCodeStyle}
        language={match[1]}
        PreTag="div"
        wrapperStyle={customWrapperStyle}
        customStyle={{
          margin: 0,
          background: 'hsl(var(--secondary))',
        }}
        className="scrollbar-custom p-4"
        {...props}
      >
        {String(children).replace(/\n$/, '')}
      </SyntaxHighlighter>
    </div>
  ) : (
    <code className="bg-secondary px-1.5 py-0.5 rounded-md text-sm" {...props}>
      {children}
    </code>
  );
};

```

## frontend/lib/utils.ts

```typescript
import { clsx, type ClassValue } from "clsx"
import { twMerge } from "tailwind-merge"

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}

```

## frontend/lib/hooks/useWebSocket.ts

```typescript
import { useState, useEffect, useCallback, useRef } from 'react';

interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
}

interface WebSocketHookReturn {
  messages: Message[];
  isLoading: boolean;
  isConnected: boolean;
  currentAgent: string;
  sendMessage: (content: string) => void;
  clearMessages: () => void;
}

export const useWebSocket = (url: string): WebSocketHookReturn => {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const [isConnected, setIsConnected] = useState(false);
  const [currentAgent, setCurrentAgent] = useState('');
  const [ws, setWs] = useState<WebSocket | null>(null);
  const reconnectTimeoutRef = useRef<NodeJS.Timeout>();

  const connectWebSocket = useCallback(() => {
    const socket = new WebSocket(url);
    setWs(socket);

    socket.onopen = () => {
      console.log('WebSocket connection established');
      setIsConnected(true);
    };

    socket.onmessage = (event) => {
      const data = JSON.parse(event.data);
      if (data.type === 'content') {
        setMessages(prevMessages => {
          const lastMessage = prevMessages[prevMessages.length - 1];
          if (lastMessage && lastMessage.role === 'assistant') {
            return [
              ...prevMessages.slice(0, -1),
              { ...lastMessage, content: lastMessage.content + data.content }
            ];
          } else {
            return [...prevMessages, {
              id: Date.now().toString(),
              role: 'assistant',
              content: data.content,
              timestamp: new Date()
            }];
          }
        });
        setIsLoading(false);
      } else if (data.type === 'agent_change') {
        setCurrentAgent(data.agent);
      } else if (data.type === 'end') {
        setIsLoading(false);
      }
    };

    socket.onclose = () => {
      console.log('WebSocket connection closed');
      setIsConnected(false);
      reconnectTimeoutRef.current = setTimeout(connectWebSocket, 5000);
    };

    socket.onerror = (error) => {
      console.error('WebSocket error:', error);
      socket.close();
    };
  }, [url]);

  useEffect(() => {
    connectWebSocket();

    return () => {
      if (ws) {
        ws.close();
      }
      if (reconnectTimeoutRef.current) {
        clearTimeout(reconnectTimeoutRef.current);
      }
    };
  }, [connectWebSocket]);

  const sendMessage = useCallback((content: string) => {
    if (!content.trim() || !ws || !isConnected) return;

    const userMessage: Message = {
      id: Date.now().toString(),
      role: 'user',
      content,
      timestamp: new Date()
    };
    setMessages(prevMessages => [...prevMessages, userMessage]);
    setIsLoading(true);

    // Get the last 4 interactions (8 messages, as each interaction has a user and assistant message)
    const lastMessages = messages.slice(-8).concat(userMessage);

    // Send the last 4 interactions along with the new message
    ws.send(JSON.stringify({
      message: content,
      history: lastMessages
    }));
  }, [ws, isConnected, messages]);

  const clearMessages = useCallback(() => {
    setMessages([]);
  }, []);

  return {
    messages,
    isLoading,
    isConnected,
    currentAgent,
    sendMessage,
    clearMessages
  };
};

export type { Message };

```

## backend/instructions.py

```typescript
triage_instructions="""
You are a highly skilled AI assistant specializing in triage. As a member of an AI team, each with specialized skills and tools, your role is to understand user inquiries and connect them to the most appropriate team member.

**Team Members:**

1. **triage_agent (Yourself):**
   - **Role:** Understands the user's question and determines the best course of action.
   - **Responsibility:** Connects the user to other team members based on the nature of the inquiry.

2. **code_agent:**
   - **Expertise:** Code interpretation and execution, proficient in shell, bash, and Python.
   - **Capabilities:** 
     - **execute_code:** Executes Python code and returns the output.
     - **read_file:** Reads files from a given path, including PDFs.
     - **install_package:** Installs Python packages into the user's virtual environment.
     - **run_python_script:** Runs Python scripts and provides the output.
   - **Use Cases:** Data analysis, algorithm development, debugging, and more.

3. **web_agent:**
   - **Expertise:** Web-related tasks and information retrieval from reputable sources.
   - **Capabilities:** 
     - **tavily_search:** Searches the internet using the Tavily client for relevant information.
     - **get_video_transcript:** Retrieves transcripts of YouTube videos.
     - **get_all_urls:** Gathers a list of connected URLs from a specified URL.
     - **get_website_text_content:** Extracts the content of a webpage.
     - **handle_research_report:** Generates deep and detailed research reports asynchronously.
   - **Use Cases:** Answering questions about current events, facts, general knowledge, web scraping, and research.

4. **reasoning_agent:**
   - **Expertise:** Advanced reasoning and problem-solving using OpenAI's latest and most advanced model, .
   - **Capabilities:**
     - **reason_with_o1:** Utilizes OpenAI's GPT-4 model for complex reasoning tasks.
   - **Use Cases:** Solving complex problems, providing detailed explanations, and handling tasks that require advanced cognitive abilities.

**Your Task:**
- Assess the user's request.
- Determine which team member is best suited to handle the request.
- Connect the user to the appropriate team member to ensure efficient and accurate assistance.
"""

web_instructions="""
You are the web_agent, a highly skilled AI assistant specializing in all web-related tasks. Utilize your tools to fulfill user requests effectively. Your capabilities include:

- **Browsing the Web:** Access and navigate websites to gather information.
- **Scraping URLs:** Extract URLs from web pages.
- **Retrieving Connected URLs:** Find all URLs linked to a specific URL.
- **Generating Research Reports:** Use the `handle_research_report` function to create comprehensive and detailed research reports asynchronously.

**Available Tools:**
- **tavily_search:** Search the internet using the Tavily client for relevant information.
- **get_video_transcript:** Obtain transcripts of YouTube videos.
- **get_all_urls:** Retrieve a list of connected URLs from a given URL.
- **get_website_text_content:** Extract the textual content from a webpage.
- **handle_research_report:** Generate in-depth research reports.

**Your Role:**
- Leverage your web expertise and tools to provide accurate and timely information.
- Ensure all responses are based on reputable sources.
- When a user requests information that requires web interaction, use the appropriate tools to gather and present the data effectively.
"""

code_instructions="""
You are the code_agent, a highly skilled AI assistant specializing in executing and interpreting code. As part of an AI team, your role is to address user queries by executing code efficiently. You have access to a robust set of tools tailored for coding tasks.

**Capabilities:**
- **Bash Command Execution:** Execute bash commands and return the output.
- **Python Code Execution:** Run Python code snippets and provide results.
- **Package Installation:** Install necessary Python packages upon user confirmation.
- **Script Execution:** Run complete Python scripts and return their outputs.

**Available Tools:**
- **execute_code:** Executes Python code and returns the output.
- **read_file:** Reads files from a specified path, including PDFs.
- **install_package:** Installs Python packages into the user's virtual environment. *Note: Always confirm with the user before installing any packages.*
- **run_python_script:** Executes Python scripts and provides the resulting output.

**Your Role:**
- Analyze the user's coding-related request.
- Determine the best approach to address the query using your available tools.
- Execute code responsibly, ensuring that any package installations are approved by the user beforehand.
- Provide clear explanations of the results to aid the user's understanding.
- Assist with tasks such as data analysis, algorithm development, debugging, and more, leveraging your coding expertise.
"""

reasoning_instructions="""
You are the reasoning_agent, a highly advanced AI assistant specializing in complex problem-solving and detailed reasoning. Your primary tool is the GPT-4 model, which you use to tackle challenging questions and provide in-depth explanations.

The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.
Learn about the capabilities and limitations of o1 models in our reasoning guide.

There are two model types available today:

o1-preview: reasoning model designed to solve hard problems across domains.
o1-mini: faster and cheaper reasoning model particularly good at coding, math, and science.

**Capabilities:**
- **Advanced Reasoning:** Utilize the o1-preview model to analyze complex problems and generate thoughtful solutions.
- **Detailed Explanations:** Provide comprehensive and clear explanations for complex topics.
- **Problem-Solving:** Break down intricate issues into manageable components and address each systematically.

**Available Tools:**
- **reason_with_o1:** Access the o1-preview model to perform advanced reasoning tasks.

**Your Role:**
- Analyze the user's complex queries or problems that require deep thinking.
- Use the o1-preview model through the `reason_with_o1` function to generate well-reasoned responses.
- Provide clear, detailed, and logically structured explanations or solutions.
- Handle tasks that require advanced cognitive abilities, such as:
  - Solving complex theoretical problems
  - Analyzing abstract concepts
  - Providing multi-step explanations for complicated processes
  - Offering nuanced perspectives on complex issues
"""

```

## backend/tools.py

```typescript
import os
import json
import sys
import venv
import subprocess
import PyPDF2 # type: ignore
import trafilatura # type: ignore
import logging
from openai import OpenAI
from typing import List, Dict, Generator, Any
from bs4 import BeautifulSoup # type: ignore
from tavily import TavilyClient # type: ignore
from urllib.parse import urljoin, urlparse
from youtube_transcript_api import YouTubeTranscriptApi # type: ignore
from gpt_researcher import GPTResearcher # type: ignore
import asyncio
import nest_asyncio  # type: ignore # Add this import

# Apply nest_asyncio to allow nested event loops
nest_asyncio.apply()

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def tavily_search(query: str) -> str:
    """
    Perform a search using the Tavily API.

    This function takes a search query and uses the Tavily client to perform a basic search.
    It returns the search context limited to a maximum of 8000 tokens.

    Args:
        query (str): The search query string.

    Returns:
        str: The search result context or an error message if the search fails.
    """
    logging.info(f"Performing Tavily search with query: {query}")
    try:
        search_result = tavily_client.get_search_context(query, search_depth="basic", max_tokens=8000)
        logging.info("Tavily search completed successfully")
        return search_result
    except Exception as e:
        error_message = f"Error performing Tavily search: {str(e)}"
        logging.error(error_message)
        return error_message



async def fetch_report(query):
    """
    Fetch a research report based on the provided query and report type.
    """
    researcher = GPTResearcher(query=query)
    await researcher.conduct_research()
    report = await researcher.write_report()
    return report

def run_async(coroutine):
    """Helper function to run async functions in a sync context."""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    return loop.run_until_complete(coroutine)

def generate_research_report(query: str) -> str:
    """
    Synchronous function to generate a research report.
    Uses the current event loop if one exists, or creates a new one if needed.
    """
    async def _async_research():
        try:
            researcher = GPTResearcher(query=query)
            await researcher.conduct_research()
            return await researcher.write_report()
        except Exception as e:
            logging.error(f"Error in _async_research: {str(e)}")
            return f"Error conducting research: {str(e)}"

    try:
        # Get the current event loop
        loop = asyncio.get_running_loop()
        # Create a new task in the current loop
        return loop.run_until_complete(_async_research())
    except RuntimeError:
        # If no event loop is running, create a new one
        return asyncio.run(_async_research())
    except Exception as e:
        logging.error(f"Error in generate_research_report: {str(e)}")
        return f"Error generating research report: {str(e)}"

def get_video_transcript(video_id):
    """
    Retrieve the transcript of a YouTube video.

    This function takes a YouTube video ID and attempts to fetch its transcript
    using the YouTubeTranscriptApi.

    Args:
        video_id (str): The ID of the YouTube video.

    Returns:
        list or str: A list containing the transcript data if successful,
                     or an error message string if the retrieval fails.
    """
    logging.info(f"Fetching transcript for video ID: {video_id}")
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id)
        logging.info("Video transcript fetched successfully")
        return transcript
    except Exception as e:
        error_message = f"An error occurred: {str(e)}"
        logging.error(error_message)
        return error_message
    
def get_website_text_content(url: str) -> str:
    """
    Fetches and extracts the main text content from a given URL.

    Args:
        url (str): The URL of the website to fetch content from.

    Returns:
        str: The extracted text content from the website.

    Raises:
        Any exceptions raised by trafilatura.fetch_url or trafilatura.extract.
    """
    logging.info(f"Fetching content from URL: {url}")
    downloaded = trafilatura.fetch_url(url)
    text = trafilatura.extract(downloaded)
    logging.info("Website content extracted successfully")
    return text

def save_to_md(text: str, filename: str) -> None:
    """
    Saves the given text content to a markdown file.

    Args:
        text (str): The text content to be saved.
        filename (str): The name of the file to save the content to.

    Returns:
        None

    Raises:
        IOError: If there's an issue writing to the file.
    """
    logging.info(f"Saving content to file: {filename}")
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(text)
        logging.info(f"Content saved successfully to {filename}")
    except IOError as e:
        logging.error(f"Error saving content to file: {str(e)}")

def get_all_urls(base_url):
    """
    Process a given URL to find all connected URLs within the same domain.

    This function downloads the content of the base_url, extracts all links,
    filters for links within the same domain and removes duplicates.

    Args:
        base_url (str): The URL to process.

    Returns:
        list: A list of unique URLs connected to the base_url within the same domain.

    Raises:
        Exception: If there's an error during URL processing.
    """
    logging.info(f"Processing URL: {base_url}")
    connected_urls = []
    try:
        downloaded = trafilatura.fetch_url(base_url)
        if downloaded is None:
            logging.warning(f"Failed to download {base_url}")
            return []

        soup = BeautifulSoup(downloaded, 'lxml')
        
        for link in soup.find_all('a', href=True):
            url = urljoin(base_url, link['href'])
            if urlparse(url).netloc == urlparse(base_url).netloc:
                connected_urls.append(url)

        # Remove duplicates
        connected_urls = list(set(connected_urls))

    except Exception as e:
        logging.error(f"Error processing URL: {str(e)}")

    return connected_urls

# Code Agent tools

def execute_command(command):
    """
    Execute a shell command and return its output.

    This function runs a given shell command using subprocess and returns the command's
    standard output. If the command fails, it returns the error message. This function has many uses. For example, performing CRUD operations, running a script, or executing a system command, using webget or curl to download a file, etc.

    Args:
        command (str): The shell command to execute.

    Returns:
        str: The command's standard output if successful, or an error message if the command fails.
    """
    logging.info(f"Executing command: {command}")
    current_dir = os.getcwd()
    workspace_dir = os.path.join(current_dir, 'WORKSPACE')
    
    try:
        os.chdir(workspace_dir)
        result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        logging.info("Command executed successfully")
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        error_message = f"Command failed with error: {e.stderr.strip()}"
        logging.error(error_message)
        return error_message
    finally:
        os.chdir(current_dir)

def read_file(file_path):
    """
    Read the contents of various file types from the WORKSPACE directory.

    Supported file types: md, txt, pdf, mdx, py, ts, tsx, js, jsx, css, scss, html
    The function automatically looks for files in the WORKSPACE directory relative to the
    current working directory.

    Args:
        file_path (str): The path to the file to be read, relative to WORKSPACE directory.

    Returns:
        str: The contents of the file.

    Raises:
        ValueError: If the file type is not supported.
        IOError: If there's an issue reading the file.
    """
    logging.info(f"Reading file: {file_path}")
    
    # Get the current directory and construct the WORKSPACE path
    current_dir = os.getcwd()
    workspace_dir = os.path.join(current_dir, 'WORKSPACE')
    
    # Construct the full file path within WORKSPACE
    full_file_path = os.path.join(workspace_dir, file_path)
    
    file_extension = os.path.splitext(full_file_path)[1].lower()
    supported_extensions = ['.md', '.txt', '.pdf', '.mdx', '.py', '.ts', '.tsx', '.js', '.jsx', '.css', '.scss', '.html']
    
    if file_extension not in supported_extensions:
        raise ValueError(f"Unsupported file type: {file_extension}")
    
    try:
        if file_extension == '.pdf':
            with open(full_file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                content = ""
                for page in pdf_reader.pages:
                    content += page.extract_text()
        else:
            with open(full_file_path, 'r', encoding='utf-8') as file:
                content = file.read()
        
        logging.info(f"File {full_file_path} read successfully")
        return content
    
    except IOError as e:
        logging.error(f"Error reading file {full_file_path}: {str(e)}")
        raise

def install_package(package_name):
    """
    Install a Python package in the /venv virtual environment.

    Args:
        package_name (str): The name of the package to install.

    Returns:
        str: The output of the installation command or an error message.
    """
    logging.info(f"Installing package: {package_name}")
    venv_path = "venv"
    pip_path = f"{venv_path}/bin/pip"
    
    if not os.path.exists(pip_path):
        error_message = f"Virtual environment not found at {venv_path}"
        logging.error(error_message)
        return error_message
    
    try:
        result = subprocess.run([pip_path, "install", package_name], 
                                check=True, 
                                stdout=subprocess.PIPE, 
                                stderr=subprocess.PIPE, 
                                text=True)
        logging.info(f"Package {package_name} installed successfully")
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        error_message = f"Failed to install package {package_name}: {e.stderr.strip()}"
        logging.error(error_message)
        return error_message

def run_python_script(filename):
    """
    Run a Python script using the Python interpreter from the /venv virtual environment.

    Args:
        filename (str): The name of the Python script to run.

    Returns:
        str: The output of the script or an error message.
    """
    logging.info(f"Running Python script: {filename}")
    venv_path = "/venv"
    python_path = f"{venv_path}/bin/python"
    
    if not os.path.exists(python_path):
        error_message = f"Virtual environment not found at {venv_path}"
        logging.error(error_message)
        return error_message
    
    try:
        result = subprocess.run([python_path, filename], 
                                check=True, 
                                stdout=subprocess.PIPE, 
                                stderr=subprocess.PIPE, 
                                text=True)
        logging.info(f"Script {filename} executed successfully")
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        error_message = f"Failed to run script {filename}: {e.stderr.strip()}"
        logging.error(error_message)
        return error_message




def reason_with_o1(
    messages: List[Dict[str, str]], 
    model: str = "gpt-4",
    client: OpenAI = None,
    **kwargs
) -> Generator[str, None, None]:
    """
    Stream chat completions from OpenAI API.
    
    Args:
        messages: List of message dictionaries with 'role' and 'content' keys
        model: OpenAI model to use (default: "gpt-4")
        client: Optional OpenAI client instance
        **kwargs: Additional parameters to pass to completion.create()
    
    Yields:
        Content chunks from the streaming response
        
    Example:
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"}
        ]
        
        for chunk in stream_chat_completion(messages):
            print(chunk, end='', flush=True)
    """
    # Create new client if none provided
    if client is None:
        client = OpenAI()
    
    # Create streaming completion
    completion = client.chat.completions.create(
        model=model,
        messages=messages,
        stream=True,
        **kwargs
    )
    
    # Yield content from chunks
    for chunk in completion:
        if chunk.choices[0].delta.content is not None:
            yield chunk.choices[0].delta.content
```

## backend/test.py

```typescript
import asyncio
import websockets # type: ignore
import json

async def test_websocket():
    uri = "ws://localhost:8000/ws"
    async with websockets.connect(uri) as websocket:
        while True:
            message = input("Enter your message (or 'quit' to exit): ")
            if message.lower() == 'quit':
                break
            
            await websocket.send(message)
            print("Message sent. Waiting for response...")

            while True:
                response = await websocket.recv()
                data = json.loads(response)
                if data['type'] == 'agent_change':
                    print(f"Agent changed to: {data['agent']}")
                elif data['type'] == 'content':
                    print(data['content'], end='', flush=True)
                elif data['type'] == 'end':
                    print(f"\nResponse ended. Final agent: {data['agent']}")
                    break

asyncio.get_event_loop().run_until_complete(test_websocket())
```

## backend/main.py

```typescript
# main.py
import os
from fastapi import FastAPI, WebSocket, WebSocketDisconnect # type: ignore
from pydantic import BaseModel
from typing import List, Dict, Any
import json
import asyncio
from swarm import Swarm, Agent # type: ignore
import nest_asyncio # type: ignore

# Apply nest_asyncio
nest_asyncio.apply()

from tools import *
from instructions import *

app = FastAPI()

client = Swarm()

MODEL = "gpt-4o-mini"

def transfer_back_to_triage():
    """Call this function to transfer back to the triage_agent."""
    return triage_agent

def transfer_to_web_agent():
   """Call this function to transfer to the web_agent."""
   return web_agent

def transfer_to_code_agent():
   """Call this function to transfer to the execute_command_agent"""
   return code_agent

def transfer_to_reasoning_agent():
    """Call this function to transfer to the reasoning_agent"""
    return reasoning_agent

triage_agent = Agent(
    name="Triage Agent",
    instructions=triage_instructions,
    functions=[transfer_to_code_agent, transfer_to_web_agent, transfer_to_reasoning_agent],
    model=MODEL,
)

web_agent = Agent(
    name="Web Agent",
    instructions=web_instructions,
    functions=[
        tavily_search, 
        get_video_transcript, 
        get_website_text_content, 
        save_to_md, 
        get_all_urls, 
        generate_research_report
    ],
    model=MODEL,
)

code_agent = Agent(
    name="Code Agent",
    instructions=code_instructions,
    functions=[execute_command, read_file, install_package, run_python_script, transfer_back_to_triage],
    model=MODEL,
)

reasoning_agent = Agent(
    name="Reasoning Agent",
    instructions=reasoning_instructions,
    functions=[reason_with_o1,transfer_back_to_triage],
    model=MODEL,
)

# Append functions to agents
triage_agent.functions.extend([transfer_to_code_agent, transfer_to_web_agent, transfer_to_reasoning_agent])
web_agent.functions.extend([transfer_back_to_triage])
code_agent.functions.extend([transfer_back_to_triage])
reasoning_agent.functions.extend([transfer_back_to_triage])
class Message(BaseModel):
    role: str
    content: str

class ConversationRequest(BaseModel):
    messages: List[Message]

@app.post("/chat")
async def chat(request: ConversationRequest):
    messages = [{"role": msg.role, "content": msg.content} for msg in request.messages]
    agent = triage_agent
    response = client.run(agent=agent, messages=messages)
    return {"response": response.messages[-1]["content"], "agent": response.agent.name}

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    agent = triage_agent

    try:
        while True:
            data = await websocket.receive_json()
            message = data['message']
            history = data.get('history', [])

            # Convert history to the format expected by the Swarm client
            messages = [{"role": msg["role"], "content": msg["content"]} for msg in history]
            messages.append({"role": "user", "content": message})

            async def process_stream():
                stream = client.run(agent=agent, messages=messages, stream=True, debug=True)
                current_agent_name = None
                for chunk in stream:
                    if isinstance(chunk, dict):
                        if 'sender' in chunk and chunk['sender'] != current_agent_name:
                            current_agent_name = chunk['sender']
                            await websocket.send_json({"type": "agent_change", "agent": current_agent_name})
                        if 'content' in chunk and chunk['content'] is not None:
                            await websocket.send_json({"type": "content", "content": chunk['content']})
                    await asyncio.sleep(0)

            # Create a new task for processing the stream
            await asyncio.create_task(process_stream())

            response = client.run(agent=agent, messages=messages)
            agent = response.agent

            await websocket.send_json({"type": "end", "agent": agent.name})

    except WebSocketDisconnect:
        print("WebSocket disconnected")

if __name__ == "__main__":
    import uvicorn # type: ignore
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

